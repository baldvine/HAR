---
title: "Human Activity Recognition"
author: "Baldvin Einarsson"
date: "6/30/2017"
output: 
    html_document:
        keep_md: TRUE

---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", cache.path = "./data/", message = FALSE)
library(knitr)
library(magrittr)
library(caret)
library(plotly)
```

Human Activity Recognition (HAR) is an interesting topic, with monitors and sensors becoming widely used. See information on original data in the following website:
[http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har)

Here, we look at the weight lifting exercise dataset. This dataset is concerned with classifying whether an a weight lifting exercise was done properly.

# Import Data

Let's download training and test sets from the following websites:

* The data is the following (which will be split into training/test for cross-validation):
[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

* The data to predict on:
[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

Let's import these data files into `R`, into objects `mainData` and `data2predict`. After a little bit of trial and error, we settle on `na.strings = c("NA","#DIV/0!")` in order to handle missing or invalid values:
```{r loadData, echo=TRUE}
mainData <- read.csv(file = "./data/pml-training.csv",
                     header = TRUE, stringsAsFactors = FALSE, 
                     na.strings = c("NA","#DIV/0!"))

data2predict <- read.csv(file = "./data/pml-testing.csv",
                     header = TRUE, stringsAsFactors = FALSE,
                     na.strings = c("NA","#DIV/0!"))
```

The data contains `r length(mainData)` columns. The columns are the same except for two columns, one from each dataset. The `mainData` contains the column "`r names(mainData)[!(names(mainData) %in% intersect(names(data2predict), names(mainData)))]`" and `data2predict` contains the column "`r names(data2predict)[!(names(data2predict) %in% intersect(names(data2predict), names(mainData)))]`".

Six people performed 10 repetitions of the "Unilateral" Dumbbell Biceps Curl", in five different ways. The variable we're interested is called `classe`, and the unique values labeled as A through E, with the following descriptions:

The classes stand for the following:
```{r describeClasses,echo=FALSE}
kable(data.frame(Class=LETTERS[1:5],
                 Description = c("Exactly according to the specification",
                                 "Throwing the elbows to the front",
                                 "Lifting the dumbbell only halfway",
                                 "Lowering the dumbbell only halfway",
                                 "Throwing the hips to the front")
                 )
      )
```

What now follows is some menial data handling, removing columns which do not contain any relevant information etc. Please feel free to skip to the [section on splitting data](#splitData) or [model building section](#modelBuilding), where the real action is.

## Remove unwanted columns

We see that the first seven columns are of no use, as they contain line numbers, name of person, time stamps and two columns on "windows". We remove these columns from the `mainData`

```{r}
mainData <- 
    mainData[,-match(c("X","user_name","raw_timestamp_part_1","raw_timestamp_part_2",
                         "cvtd_timestamp","new_window","num_window"),
                     names(mainData))]
```

We also notice that some columns contain only NA values, or a significant amount of NAs. Let's inspect further:
```{r}
countNAs <- sapply(mainData, function(x){sum(is.na(x))})
```
The columns with no NAs are the actual measurements, and they have the following prefixes:
```{r}
regexpr("^[A-Za-z]+", names(countNAs[countNAs == 0])) %>% 
    regmatches(names(countNAs[countNAs == 0]), m = .) %>% 
    unique()
```
The minimum number of NAs in those columns which do have NA values, is `r min(countNAs[countNAs > 0])` (out of `r nrow(mainData)`). These columns are the derived quantities such as max, min, average, variance etc. In fact, all these NA columns have the following prefixes:
```{r}
regexpr("^[A-Za-z]+", names(countNAs[countNAs > 0])) %>% 
    regmatches(names(countNAs[countNAs > 0]), m = .) %>% 
    unique()
```
Let's remove these columns with derived valued and a bunch of NAs:
```{r}
mainData <- mainData[,names(countNAs[countNAs == 0])]
```


Ok, so now all the columns apart the last one (the classifier), are numerical values:
```{r}
sapply(mainData[,-length(mainData)], class) %>% unique
```


This brings us down to `r length(mainData)-1` predictors, all numeric inputs. From now on we expect any predictive algorithm to be able to find important columns, and we stop cleaning the data.

## Split into training and test set {#splitData}

For cross-validation, let's split the data into a training and test set randomly with 60% and 40% of the initial data each, respectively.

```{r splitTrainTest}
set.seed(42)
inTrain <- createDataPartition(mainData$classe, p = 0.6)[[1]]
training <- mainData[inTrain,]
test <- mainData[-inTrain,]
```

# Building predictive models {#modelBuilding}

Here, it is easy to go crazy and have lots of fun with various classification models. Let's contain ourselves and only try a few for now. We'll create a list of model fits, predictions and confusion matrices, whose element names correspond to the model used.
```{r initLists, echo=FALSE}
modelFit <- list()          # the model fit object
training.pred <- list()     # prediction on the training set
test.pred <- list()         # prediction on the test set
confMat.training <- list()  # confusion matrix on training set
confMat.test <- list()      # confusion matrix on test set
```


```{r setpcaThresh, echo=FALSE}

# Make sure that chunks which use this have 'dependson="setpcaThresh"'
pcaThresh <- 0.90

```


First, let's try linear discriminant analysis, LDA:
```{r fit.lda, message=FALSE, cache=TRUE}
myMethod <- "lda"
modelFit[[myMethod]] <- 
  train(classe~., method = myMethod, data = training, na.action = na.omit)

# Predict on both training and test set:
training.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = training)
test.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = test)

# Obtain confusion matrices for both sets:
confMat.training[[myMethod]] <- confusionMatrix(training.pred[[myMethod]], training$classe)
confMat.test[[myMethod]] <- confusionMatrix(test.pred[[myMethod]], test$classe)
```

Let's obtain the predictions and confusion matrices for gradient boosting (method "gbm") and random forests (method "rf"). Since the change from the above chunk only involves changing the method name and minor changes, we don't show the code. Note that the code is available in the `.Rmd` file.

```{r fit.gbm, echo=FALSE, message=FALSE, cache=TRUE}
myMethod <- "gbm"
modelFit[[myMethod]] <- 
  train(classe~., method = myMethod, data = training, na.action = na.omit, verbose=FALSE)
training.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = training)
test.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = test)
confMat.training[[myMethod]] <- confusionMatrix(training.pred[[myMethod]], training$classe)
confMat.test[[myMethod]] <- confusionMatrix(test.pred[[myMethod]], test$classe)
```

```{r fit.gbm.wPCA, echo=FALSE, message=FALSE, cache=TRUE, dependson="setpcaThresh"}
myMethod <- "gbm.wPCA"
modelFit[[myMethod]] <- 
  train(classe~., method = gsub("\\.[A-Za-z]+$","",myMethod),  
        data = training, preProcess = "pca", 
        trControl = trainControl(preProcOptions = list(thresh = pcaThresh)),
        na.action = na.omit, verbose = FALSE)
training.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = training)
test.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = test)
confMat.training[[myMethod]] <- confusionMatrix(training.pred[[myMethod]], training$classe)
confMat.test[[myMethod]] <- confusionMatrix(test.pred[[myMethod]], test$classe)
```

```{r fit.rf, echo=FALSE, cache=TRUE}
myMethod <- "rf"
modelFit[[myMethod]] <- 
  train(classe~., method = myMethod, data = training, na.action = na.omit)
training.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = training)
test.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = test)
confMat.training[[myMethod]] <- confusionMatrix(training.pred[[myMethod]], training$classe)
confMat.test[[myMethod]] <- confusionMatrix(test.pred[[myMethod]], test$classe)
```

```{r fit.rf.wPCA, echo=FALSE, cache=TRUE, warning=FALSE, dependson="setpcaThresh"}
myMethod <- "rf.wPCA"
modelFit[[myMethod]] <- 
  train(classe~., method = gsub("\\.[A-Za-z]+$","",myMethod),  
        data = training, preProcess = "pca", 
        trControl = trainControl(preProcOptions = list(thresh = pcaThresh)),
        na.action = na.omit)
training.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = training)
test.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = test)
confMat.training[[myMethod]] <- confusionMatrix(training.pred[[myMethod]], training$classe)
confMat.test[[myMethod]] <- confusionMatrix(test.pred[[myMethod]], test$classe)
```

Note that since gradient boosting and random forests are usually very computationally heavy, we also preprocess with principal components (with label having suffix ".wPCA"), explaining `r sprintf(fmt = "%2.1f", 100*pcaThresh)` percent of the variance. It is also interesting to compare the in and out of sample errors with or without preprocessing.

## Inspecting accuracy and in and out of sample errors

Let's now do some basic check on how well the metods above performed. To this end, we use the aptly named object "Accuracy", in the `overall` element of the confusion matrices. For all methods used we inspect the accuracy on both the training and the test set:

```{r showAccuracy, echo=FALSE}
kable(data.frame(Method = names(modelFit), 
                 Accuracy.train = sapply(X = confMat.training, 
                                         FUN = function(x){x$overall["Accuracy"]}),
                 Accuracy.test  = sapply(X = confMat.test, 
                                         FUN = function(x){x$overall["Accuracy"]})
                 ), 
      col.names = c("Method","Accuracy on training set", "Accuracy on test set"),
      row.names = FALSE, format.args = list(digits = 3))

```

It is interesting to note that both models using random forests have perfect predictions on the training set. However good that may sound, it smells like overfitting. Our goal should not be a perfect fit on the training set, but rather obtain some kind of balance between variance and bias of our model fit.


For that reason, I would prefer gradient boosting (method "gbm"). Let's visualize its confusion matrix using plotly (**not shown, but the R-markdown has code for those interested**):
```{r visConfMatrix, echo=FALSE}
# Select method and graph label:
selectedMethod <- "gbm"
selectedMethod.label <- "gradient boosting"

# First, we have to do some basic data handling to get a data frame:
myConfMat <- as.matrix(confMat.test[[selectedMethod]]$table) %*% diag(1/table(test$classe))

# Fix the dimname issue:
dimnames(myConfMat)[[2]] <- dimnames(myConfMat)$Prediction
names(dimnames(myConfMat))[2] <- "Reference"

# Melt for plotting in ggplot2:
myConfDF <- reshape2::melt(myConfMat, value.name = "Ratio")

# Set the color scale:
# From https://plot.ly/r/heatmaps/
vals <- unique(scales::rescale(c(myConfMat)))
o <- order(vals, decreasing = FALSE)
cols <- scales::col_numeric("Blues", domain = NULL)(vals)
colz <- setNames(data.frame(vals[o], cols[o]), NULL)

# Woot woot!
plot_ly(data = myConfDF,
        x = myConfDF$Reference,
        y = myConfDF$Prediction,
        z = myConfDF$Ratio^0.5, # Just for visualization; tooltip has correct value
        type = 'heatmap',
        colorscale = colz,
        showscale = FALSE,
        hoverinfo = "text",
        text = ~paste("Prediction: ", Prediction,
                      "<br>Reference: ", Reference,
                      "<br>Percentage predicted: ", sprintf(fmt = "%2.1f", 100*Ratio),"%")
        ) %>% 
    layout(title = paste0("Confusion matrix for ", selectedMethod.label),
           xaxis = list(title = "Reference"), 
           yaxis = list(title = "Prediction")
           )

```

```{r vizConfMat_ggplot,echo=FALSE, eval=FALSE}
# NB: Color scale needs to be made to look "nice"
plot.gg <- 
    ggplot(data = myConfDF, 
           aes(x = Reference, y = Prediction)) +
    geom_tile(aes(fill = Ratio)) +
    scale_fill_gradient2(low = "red", mid = "yellow", high = "green", midpoint = 0.01) + 
    ggtitle(label = "Confusion matrix for random forests") +
    theme(plot.title = element_text(hjust = 0.5))

```

# Prediction

Finally, we predict the values on the prediction data set, `data2predict`, using the `r selectedMethod` model fit above. The data contains 20 rows, and so the output is a vector of length 20:
```{r}

print(selectedMethod)  # Value was set above
predict(object = modelFit[[selectedMethod]], newdata = data2predict)

```


