---
title: "Human Activity Recognition"
author: "Baldvin Einarsson"
date: "6/3/2017"
output: 
    html_document:
        keep_md: TRUE

---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment="", cache.path = "./data/", message = FALSE)
library(knitr)
library(magrittr)
library(caret)
library(plotly)
```

Human Activity Recognition (HAR) is an interesting topic, with monitors and sensors becoming widely used. See information on original data in the following website:
[http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har)

Here, we look at the weight lifting exercise dataset. This dataset is concerned with classifying whether an a weight lifting exercise was done properly.

# Import Data

Let's download training and test sets from the following websites:

* The data is the following (which will be split into training/test for cross-validation):
[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

* The data to predict on:
[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

Let's import these data files into `R`, into objects `mainData` and `data2predict`. After a little bit of trial and error, we settle on `na.strings = c("NA","#DIV/0!")` in order to handle missing or invalid values:
```{r loadData, echo=TRUE}
mainData <- read.csv(file = "./data/pml-training.csv",
                     header = TRUE, stringsAsFactors = FALSE, 
                     na.strings = c("NA","#DIV/0!"))

data2predict <- read.csv(file = "./data/pml-testing.csv",
                     header = TRUE, stringsAsFactors = FALSE,
                     na.strings = c("NA","#DIV/0!"))
```

The data contains `r length(mainData)` columns. The columns are the same except for two columns, one from each dataset. The `mainData` contains the column "`r names(mainData)[!(names(mainData) %in% intersect(names(data2predict), names(mainData)))]`" and "`data2predict` contains the column `r names(data2predict)[!(names(data2predict) %in% intersect(names(data2predict), names(mainData)))]`".

Six people performed 10 repetitions of the "Unilateral" Dubbell Biceps Curl", in five different ways. The variable we're interested is called `classe`, and the unique values labeled as A through E, with the following descriptions:

The classes stand for the following:
```{r describeClasses,echo=FALSE}
kable(data.frame(Class=LETTERS[1:5],
                 Description = c("Exactly according to the specification",
                                 "Throwing the elbows to the front",
                                 "Lifting the dumbbell only halfway",
                                 "Lowering the dumbbell only halfway",
                                 "Throwing the hips to the front")
                 )
      )
```

## Remove unwanted columns

We see that the first seven columns are of no use, as they contain line numbers, name of person, time stamps and two columns on "windows". We remove these columns from the `mainData`

```{r}
mainData <- 
    mainData[,-match(c("X","user_name","raw_timestamp_part_1","raw_timestamp_part_2",
                         "cvtd_timestamp","new_window","num_window"),
                     names(mainData))]
```

We also notice that some columns contain only NA values, or a significant amount of NAs. Let's inspect further:
```{r}
countNAs <- sapply(mainData, function(x){sum(is.na(x))})
```
The columns with no NAs are the actual measurements, and they have the following prefixes:
```{r}
regexpr("^[A-Za-z]+", names(countNAs[countNAs == 0])) %>% 
    regmatches(names(countNAs[countNAs == 0]), m = .) %>% 
    unique()
```
The minimum number of NAs in those columns which do have NA values, is `r min(countNAs[countNAs > 0])` (out of `r nrow(mainData)`). These columns are the derived quantities such as max, min, average, variance etc. In fact, all these NA columns have the following prefixes:
```{r}
regexpr("^[A-Za-z]+", names(countNAs[countNAs > 0])) %>% 
    regmatches(names(countNAs[countNAs > 0]), m = .) %>% 
    unique()
```
Let's remove these columns with derived valued and a bunch of NAs:
```{r}
mainData <- mainData[,names(countNAs[countNAs == 0])]
```


Ok, so now all the columns apart the last one (the classifier), are numerical values:
```{r}
sapply(mainData[,-length(mainData)], class) %>% unique
```


This brings us down to `r length(mainData)-1` predictors, all numeric inputs. From now on we expect any predictive algorithm to be able to find important columns, and we stop cleaning the data.

## Split into training and test set

For cross-validation, let's split the data into a training and test set randomly with 60% and 40% of the initial data each, respectively.

```{r splitTrainTest}
set.seed(42)
inTrain <- createDataPartition(mainData$classe, p = 0.6)[[1]]
training <- mainData[inTrain,]
test <- mainData[-inTrain,]
```

# Building predictive models

Here, it is easy to go crazy and have lots of fun with various classification models. Let's contain ourselves and only try a few for now. We'll create a list of model fits and predictions in objects `modelFit` and `test.pred`, whose names correspond to the model used. We start by setting up empty lists:
```{r initLists}
modelFit <- list()
test.pred <- list()
confMat <- list()
```


First, we try linear discriminant analysis, LDA:
```{r fit.lda, message=FALSE}
myMethod <- "lda"
modelFit[[myMethod]] <- 
  train(classe~., method = myMethod, data = training, na.action = na.omit)
#training.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = training)
test.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = test)
#confusionMatrix(training.pred.lda, training$classe)
confMat[[myMethod]] <- confusionMatrix(test.pred[[myMethod]], test$classe)
```

Let's obtain the same predictions and confusion matrices for gradient boosting (method "gbm") and random forests (method "rf"). Since the change from the above chunk only involves changing the method name, we don't bother showing the output.

```{r fit.gbm, echo=FALSE, message=FALSE, cache=TRUE}
myMethod <- "gbm"
modelFit[[myMethod]] <- 
  train(classe~., method = myMethod, data = training, na.action = na.omit, verbose=FALSE)
#training.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = training)
test.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = test)
#confusionMatrix(training.pred.lda, training$classe)
confMat[[myMethod]] <- confusionMatrix(test.pred[[myMethod]], test$classe)
```

```{r fit.rf, echo=FALSE, cache=TRUE}
myMethod <- "rf"
modelFit[[myMethod]] <- 
  train(classe~., method = myMethod, data = training, na.action = na.omit)
#training.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = training)
test.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = test)
#confusionMatrix(training.pred.lda, training$classe)
confMat[[myMethod]] <- confusionMatrix(test.pred[[myMethod]], test$classe)
```

```{r fit.rf.wPCA, echo=FALSE, cache=TRUE}
myMethod <- "rf.wPCA"
pcaThresh <- 0.90
modelFit[[myMethod]] <- 
  train(classe~., method = gsub("\\.[A-Za-z]+$","",myMethod),  
        data = training, preProcess = "pca", 
        trControl = trainControl(preProcOptions = list(thresh = pcaThresh)),
        na.action = na.omit)
#training.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = training)
test.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = test)
#confusionMatrix(training.pred.lda, training$classe)
confMat[[myMethod]] <- confusionMatrix(test.pred[[myMethod]], test$classe)
```

## Inspecting accuracy

Let's now do some basic check on how well the metods above performed. To this end, we use the aptly named object 'Accuracy', in the `overall` element of the confusion matrices:

```{r showAccuracy, echo=FALSE}
lapply(X = confMat, FUN = function(x){x$overall["Accuracy"]})
```

So far, the random forests seem to be the best option in our case. Let's visualize its confusion matrix. First, we have to do some basic data handling to get a data frame:
```{r createConfMatDF}
myMethod <- "rf.wPCA"
myConfMat <- as.matrix(confMat[[myMethod]]$table) %*% diag(1/table(test$classe))
# Fix the dimname issue:
dimnames(myConfMat)[[2]] <- dimnames(myConfMat)$Prediction
names(dimnames(myConfMat))[2] <- "Reference"
# Melt for plotting in ggplot2:
myConfDF <- reshape2::melt(myConfMat, value.name = "Ratio")

```

```{r echo=FALSE, eval=FALSE}
# NB: Color scale needs to be made to look "nice"
plot.gg <- 
    ggplot(data = myConfDF, 
           aes(x = Reference, y = Prediction)) +
    geom_tile(aes(fill = Ratio)) +
    scale_fill_gradient2(low = "red", mid = "yellow", high = "green", midpoint = 0.01) + 
    ggtitle(label = "Confusion matrix for random forests") +
    theme(plot.title = element_text(hjust = 0.5))

```

Then, we visualize using plotly (not shown, but markdown has code for those interested):
```{r visConfMatrix, echo=FALSE}
# Set the color scale:
# From https://plot.ly/r/heatmaps/
vals <- unique(scales::rescale(c(myConfMat)))
o <- order(vals, decreasing = FALSE)
cols <- scales::col_numeric("Blues", domain = NULL)(vals)
colz <- setNames(data.frame(vals[o], cols[o]), NULL)

plot_ly(data = myConfDF,
        x = myConfDF$Reference,
        y = myConfDF$Prediction,
        z = myConfDF$Ratio^0.5, # Just for visualization; tooltip has correct value
        type = 'heatmap',
        colorscale = colz,
        showscale = FALSE,
        hoverinfo = "text",
        text = ~paste("Prediction: ", Prediction,
                      "<br>Reference: ", Reference,
                      "<br>Percentage predicted: ", sprintf(fmt = "%2.3f", 100*Ratio),"%")
        ) %>% 
    layout(title = "Confusion matrix for random forests & PCA",
           xaxis = list(title = "Reference"), 
           yaxis = list(title = "Prediction")
           )

```

## In and out of sample errors

To get a basic feel for whether we overfit the data or not, we compare the accuracy of random forests to that on the training set and on the test set.

```{r}
training.pred <- list()
myMethod <- "rf.wPCA"
training.pred[[myMethod]] <- predict(object = modelFit[[myMethod]], newdata = training)

confusionMatrix(training.pred[[myMethod]], training$classe)$overall["Accuracy"]
```

# Prediction

Finally, we predict the values on the prediction data set, `data2predict`, using the random forests model fit above. The data contains 20 rows, and so the output is a vector of length 20:
```{r}
myMethod <- "rf.wPCA"
predict(object = modelFit[[myMethod]], newdata = data2predict)

```


